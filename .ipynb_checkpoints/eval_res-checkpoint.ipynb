{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ad1941f-2905-4f6b-a31a-febefe6dd25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Loading normalization statistics...\n",
      "\n",
      "================================================================================\n",
      "EVALUATING RESIDUAL MODELS\n",
      "================================================================================\n",
      "\n",
      "HISTORICAL Scenario\n",
      "----------------------------------------\n",
      "\n",
      "  Variable: pr\n",
      "    Evaluating raw... Success\n",
      "    Evaluating gma... Success\n",
      "    Evaluating grid... Success\n",
      "    Evaluating gmt... Success\n",
      "\n",
      "  Variable: tas\n",
      "    Evaluating raw... Success\n",
      "    Evaluating gma... Success\n",
      "    Evaluating grid... Success\n",
      "    Evaluating gmt... Success\n",
      "\n",
      "SSP126 Scenario\n",
      "----------------------------------------\n",
      "\n",
      "  Variable: pr\n",
      "    Evaluating raw... Success\n",
      "    Evaluating gma... Success\n",
      "    Evaluating grid... Success\n",
      "    Evaluating gmt... Success\n",
      "\n",
      "  Variable: tas\n",
      "    Evaluating raw... Success\n",
      "    Evaluating gma... Success\n",
      "    Evaluating grid... Success\n",
      "    Evaluating gmt... Success\n",
      "\n",
      "SSP245 Scenario\n",
      "----------------------------------------\n",
      "\n",
      "  Variable: pr\n",
      "    Evaluating raw... Success\n",
      "    Evaluating gma... Success\n",
      "    Evaluating grid... Success\n",
      "    Evaluating gmt... Success\n",
      "\n",
      "  Variable: tas\n",
      "    Evaluating raw... Success\n",
      "    Evaluating gma... Success\n",
      "    Evaluating grid... Success\n",
      "    Evaluating gmt... Success\n",
      "\n",
      "SSP585 Scenario\n",
      "----------------------------------------\n",
      "\n",
      "  Variable: pr\n",
      "    Evaluating raw... Success\n",
      "    Evaluating gma... Success\n",
      "    Evaluating grid... Success\n",
      "    Evaluating gmt... Success\n",
      "\n",
      "  Variable: tas\n",
      "    Evaluating raw... Success\n",
      "    Evaluating gma... Success\n",
      "    Evaluating grid... Success\n",
      "    Evaluating gmt... Success\n",
      "\n",
      "================================================================================\n",
      "SAVING RESULTS\n",
      "================================================================================\n",
      "Saved: evaluation_results_residual/pr_evaluation_historical.nc\n",
      "Saved: evaluation_results_residual/tas_evaluation_historical.nc\n",
      "Saved: evaluation_results_residual/pr_evaluation_ssp126.nc\n",
      "Saved: evaluation_results_residual/tas_evaluation_ssp126.nc\n",
      "Saved: evaluation_results_residual/pr_evaluation_ssp245.nc\n",
      "Saved: evaluation_results_residual/tas_evaluation_ssp245.nc\n",
      "Saved: evaluation_results_residual/pr_evaluation_ssp585.nc\n",
      "Saved: evaluation_results_residual/tas_evaluation_ssp585.nc\n",
      "\n",
      "Evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from unet import UNet\n",
    "from scipy import stats\n",
    "\n",
    "# Configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data_dir = Path(\"data\")\n",
    "ckpt_dir = Path(\"ckpts\")\n",
    "\n",
    "# Test periods\n",
    "test_periods = {\n",
    "    'historical': ('2001', '2014'),\n",
    "    'ssp126': ('2015', '2100'),\n",
    "    'ssp245': ('2015', '2100'), \n",
    "    'ssp585': ('2015', '2100')\n",
    "}\n",
    "\n",
    "# Variables and input types to evaluate\n",
    "variables = ['pr', 'tas']\n",
    "input_types = ['raw', 'gma', 'grid', 'gmt']  # Simplified names\n",
    "\n",
    "# Load datasets\n",
    "print(\"Loading datasets...\")\n",
    "datasets = {\n",
    "    'historical': xr.open_dataset(data_dir / \"MPI-ESM1-2-HR-LR_historical_residual_detrended.nc\"),\n",
    "    'ssp126': xr.open_dataset(data_dir / \"MPI-ESM1-2-HR-LR_ssp126_residual_detrended.nc\"),\n",
    "    'ssp245': xr.open_dataset(data_dir / \"MPI-ESM1-2-HR-LR_ssp245_residual_detrended.nc\"),\n",
    "    'ssp585': xr.open_dataset(data_dir / \"MPI-ESM1-2-HR-LR_ssp585_residual_detrended.nc\")\n",
    "}\n",
    "\n",
    "# Load original datasets for undetrended versions\n",
    "datasets_original = {\n",
    "    'historical': xr.open_dataset(data_dir / \"MPI-ESM1-2-HR-LR_historical_r1i1p1f1_1850_2014_allvars.nc\"),\n",
    "    'ssp126': xr.open_dataset(data_dir / \"MPI-ESM1-2-HR-LR_ssp126_r1i1p1f1_2015_2100_allvars.nc\"),\n",
    "    'ssp245': xr.open_dataset(data_dir / \"MPI-ESM1-2-HR-LR_ssp245_r1i1p1f1_2015_2100_allvars.nc\"),\n",
    "    'ssp585': xr.open_dataset(data_dir / \"MPI-ESM1-2-HR-LR_ssp585_r1i1p1f1_2015_2100_allvars.nc\")\n",
    "}\n",
    "\n",
    "# Load normalization statistics\n",
    "print(\"Loading normalization statistics...\")\n",
    "with open(data_dir / \"norm_stats_zscore_pixel_residual_detrended.pkl\", 'rb') as f:\n",
    "    norm_stats = pickle.load(f)\n",
    "\n",
    "def evaluate_model(model_path, var, input_type, dataset, dataset_original, test_period):\n",
    "    \"\"\"Evaluate a single model on a dataset.\"\"\"\n",
    "    \n",
    "    # Load model\n",
    "    model = UNet(in_channels=1, out_channels=1, initial_features=32, depth=5, dropout=0.2)\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Get variable names based on input type\n",
    "    if input_type == 'raw':\n",
    "        input_var = f\"{var}_lr_interp\"\n",
    "        input_key = 'lr_interp'\n",
    "    else:\n",
    "        # Map simplified names back to full detrend names\n",
    "        detrend_map = {'gma': 'detrend_gma', 'grid': 'detrend_grid', 'gmt': 'detrend_gmt'}\n",
    "        input_var = f\"{var}_lr_{detrend_map[input_type]}\"\n",
    "        input_key = f'lr_{detrend_map[input_type]}'\n",
    "    \n",
    "    # Extract test data\n",
    "    lr_data = dataset[input_var].sel(time=slice(test_period[0], test_period[1])).values\n",
    "    residual_true = dataset[f\"{var}_residual\"].sel(time=slice(test_period[0], test_period[1])).values\n",
    "    lr_original = dataset_original[f\"{var}_lr_interp\"].sel(time=slice(test_period[0], test_period[1])).values\n",
    "    hr_original = dataset_original[f\"{var}_hr\"].sel(time=slice(test_period[0], test_period[1])).values\n",
    "    \n",
    "    # Apply zscore_pixel normalization to input\n",
    "    lr_mean = norm_stats[var][input_key]['pixel_mean']\n",
    "    lr_std = norm_stats[var][input_key]['pixel_std']\n",
    "    lr_normalized = (lr_data - lr_mean) / (lr_std + 1e-8)\n",
    "    \n",
    "    # Predict in batches\n",
    "    batch_size = 32\n",
    "    n_samples = len(lr_normalized)\n",
    "    predictions_norm = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, n_samples, batch_size):\n",
    "            batch = lr_normalized[i:i+batch_size]\n",
    "            batch_tensor = torch.tensor(batch, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "            batch_pred = model(batch_tensor)\n",
    "            predictions_norm.append(batch_pred.cpu().numpy())\n",
    "    \n",
    "    predictions_norm = np.concatenate(predictions_norm, axis=0).squeeze(1)\n",
    "    \n",
    "    # Denormalize residual predictions\n",
    "    residual_mean = norm_stats[var]['residual']['pixel_mean']\n",
    "    residual_std = norm_stats[var]['residual']['pixel_std']\n",
    "    residual_pred = predictions_norm * residual_std + residual_mean\n",
    "    \n",
    "    # Reconstruct full HR prediction by adding back LR_interp\n",
    "    hr_pred = residual_pred + lr_original\n",
    "    \n",
    "    return hr_pred, hr_original, lr_original, residual_pred, residual_true\n",
    "\n",
    "# Main evaluation loop\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATING RESIDUAL MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for scenario_name in datasets.keys():\n",
    "    print(f\"\\n{scenario_name.upper()} Scenario\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    dataset = datasets[scenario_name]\n",
    "    dataset_original = datasets_original[scenario_name]\n",
    "    test_period = test_periods[scenario_name]\n",
    "    scenario_results = {}\n",
    "    \n",
    "    for var in variables:\n",
    "        print(f\"\\n  Variable: {var}\")\n",
    "        var_results = {}\n",
    "        \n",
    "        # Get coordinates for creating xarray DataArrays\n",
    "        time_coords = dataset[f\"{var}_hr\"].sel(time=slice(test_period[0], test_period[1])).time\n",
    "        lat_coords = dataset[f\"{var}_hr\"].lat\n",
    "        lon_coords = dataset[f\"{var}_hr\"].lon\n",
    "        \n",
    "        # Store ground truth (original HR, not residual)\n",
    "        hr_true = dataset_original[f\"{var}_hr\"].sel(time=slice(test_period[0], test_period[1]))\n",
    "        lr_input = dataset_original[f\"{var}_lr_interp\"].sel(time=slice(test_period[0], test_period[1]))\n",
    "        \n",
    "        var_results['groundtruth'] = hr_true\n",
    "        var_results['input'] = lr_input\n",
    "        \n",
    "        for input_type in input_types:\n",
    "            # Build model filename based on your naming convention\n",
    "            if input_type == 'raw':\n",
    "                model_filename = f\"{var}_lr_to_residual.pth\"\n",
    "            else:\n",
    "                model_filename = f\"{var}_lr_{input_type}_to_residual.pth\"\n",
    "            \n",
    "            model_path = ckpt_dir / model_filename\n",
    "            \n",
    "            if not model_path.exists():\n",
    "                print(f\"    Model not found: {model_path}\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                print(f\"    Evaluating {input_type}...\", end=\" \")\n",
    "                \n",
    "                hr_pred, hr_true_np, lr_orig, residual_pred, residual_true = evaluate_model(\n",
    "                    model_path, var, input_type, dataset, dataset_original, test_period\n",
    "                )\n",
    "                \n",
    "                # Create xarray DataArray for predictions\n",
    "                pred_da = xr.DataArray(\n",
    "                    hr_pred,\n",
    "                    coords={'time': time_coords, 'lat': lat_coords, 'lon': lon_coords},\n",
    "                    dims=['time', 'lat', 'lon'],\n",
    "                    name=f'{var}_pred_{input_type}'\n",
    "                )\n",
    "                \n",
    "                var_results[f'pred_{input_type}'] = pred_da\n",
    "                print(\"Success\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                continue\n",
    "        \n",
    "        scenario_results[var] = var_results\n",
    "    \n",
    "    all_results[scenario_name] = scenario_results\n",
    "\n",
    "# Save results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "output_dir = Path(\"evaluation_results_residual\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for scenario_name, scenario_results in all_results.items():\n",
    "    for var, var_results in scenario_results.items():\n",
    "        ds_result = xr.Dataset()\n",
    "        \n",
    "        # Store ground truth and input\n",
    "        ds_result['groundtruth'] = var_results['groundtruth']\n",
    "        ds_result['input'] = var_results['input']\n",
    "        \n",
    "        # Store predictions\n",
    "        for input_type in input_types:\n",
    "            key = f'pred_{input_type}'\n",
    "            if key in var_results:\n",
    "                ds_result[key] = var_results[key]\n",
    "        \n",
    "        # Save to netCDF\n",
    "        output_path = output_dir / f\"{var}_evaluation_{scenario_name}.nc\"\n",
    "        ds_result.to_netcdf(output_path)\n",
    "        print(f\"Saved: {output_path}\")\n",
    "\n",
    "print(\"\\nEvaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fde7a0e6-4653-4a7c-bfa4-dd61eac99c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Loading normalization statistics...\n",
      "Normalization stats loaded for variables: ['pr', 'tas', 'hurs', 'sfcWind']\n",
      "\n",
      "================================================================================\n",
      "EVALUATING PRECIPITATION (PR) MODELS\n",
      "================================================================================\n",
      "\n",
      "HISTORICAL Scenario\n",
      "----------------------------------------\n",
      "    Evaluating none...\n",
      "Success\n",
      "    Evaluating minmax_global...\n",
      "Success\n",
      "    Evaluating minmax_pixel...\n",
      "Success\n",
      "    Evaluating zscore_global...\n",
      "Success\n",
      "    Evaluating zscore_pixel...\n",
      "Success\n",
      "    Evaluating instance_zscore...\n",
      "Success\n",
      "    Evaluating instance_minmax...\n",
      "Success\n",
      "\n",
      "SSP126 Scenario\n",
      "----------------------------------------\n",
      "    Evaluating none...\n",
      "Success\n",
      "    Evaluating minmax_global...\n",
      "Success\n",
      "    Evaluating minmax_pixel...\n",
      "Success\n",
      "    Evaluating zscore_global...\n",
      "Success\n",
      "    Evaluating zscore_pixel...\n",
      "Success\n",
      "    Evaluating instance_zscore...\n",
      "Success\n",
      "    Evaluating instance_minmax...\n",
      "Success\n",
      "\n",
      "SSP245 Scenario\n",
      "----------------------------------------\n",
      "    Evaluating none...\n",
      "Success\n",
      "    Evaluating minmax_global...\n",
      "Success\n",
      "    Evaluating minmax_pixel...\n",
      "Success\n",
      "    Evaluating zscore_global...\n",
      "Success\n",
      "    Evaluating zscore_pixel...\n",
      "Success\n",
      "    Evaluating instance_zscore...\n",
      "Success\n",
      "    Evaluating instance_minmax...\n",
      "Success\n",
      "\n",
      "SSP585 Scenario\n",
      "----------------------------------------\n",
      "    Evaluating none...\n",
      "Success\n",
      "    Evaluating minmax_global...\n",
      "Success\n",
      "    Evaluating minmax_pixel...\n",
      "Success\n",
      "    Evaluating zscore_global...\n",
      "Success\n",
      "    Evaluating zscore_pixel...\n",
      "Success\n",
      "    Evaluating instance_zscore...\n",
      "Success\n",
      "    Evaluating instance_minmax...\n",
      "Success\n",
      "\n",
      "================================================================================\n",
      "SAVING RESULTS\n",
      "================================================================================\n",
      "Saved: evaluation_results/pr_evaluation_historical.nc\n",
      "Saved: evaluation_results/pr_evaluation_ssp126.nc\n",
      "Saved: evaluation_results/pr_evaluation_ssp245.nc\n",
      "Saved: evaluation_results/pr_evaluation_ssp585.nc\n",
      "\n",
      "Evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "# evaluate_pr.py\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from unet import UNet\n",
    "\n",
    "# ----------------------------\n",
    "# Configuration\n",
    "# ----------------------------\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data_dir = Path(\"data\")\n",
    "ckpt_dir = Path(\"ckpts\")\n",
    "\n",
    "# Variable configuration\n",
    "variable = 'pr_hr'\n",
    "var_base = 'pr'\n",
    "var_lr = 'pr_lr_interp'\n",
    "\n",
    "# Models to evaluate\n",
    "normalizations = ['none', 'minmax_global', 'minmax_pixel', 'zscore_global', \n",
    "                 'zscore_pixel', 'instance_zscore', 'instance_minmax']\n",
    "\n",
    "# Test periods\n",
    "test_periods = {\n",
    "    'historical': ('2001', '2014'),\n",
    "    'ssp126': ('2015', '2100'),\n",
    "    'ssp245': ('2015', '2100'), \n",
    "    'ssp585': ('2015', '2100')\n",
    "}\n",
    "\n",
    "# ----------------------------\n",
    "# Load data \n",
    "# ----------------------------\n",
    "print(\"Loading datasets...\")\n",
    "ds_hist = xr.open_dataset(data_dir / \"MPI-ESM1-2-HR-LR_historical_r1i1p1f1_1850_2014_allvars.nc\")\n",
    "ds_ssp126 = xr.open_dataset(data_dir / \"MPI-ESM1-2-HR-LR_ssp126_r1i1p1f1_2015_2100_allvars.nc\")\n",
    "ds_ssp245 = xr.open_dataset(data_dir / \"MPI-ESM1-2-HR-LR_ssp245_r1i1p1f1_2015_2100_allvars.nc\")\n",
    "ds_ssp585 = xr.open_dataset(data_dir / \"MPI-ESM1-2-HR-LR_ssp585_r1i1p1f1_2015_2100_allvars.nc\")\n",
    "\n",
    "datasets = {\n",
    "    'historical': ds_hist,\n",
    "    'ssp126': ds_ssp126,\n",
    "    'ssp245': ds_ssp245,\n",
    "    'ssp585': ds_ssp585\n",
    "}\n",
    "\n",
    "# Load normalization statistics\n",
    "print(\"Loading normalization statistics...\")\n",
    "with open(data_dir / \"norm_stats.pkl\", 'rb') as f:\n",
    "    norm_stats = pickle.load(f)\n",
    "\n",
    "print(\"Normalization stats loaded for variables:\", list(norm_stats.keys()))\n",
    "\n",
    "# ----------------------------\n",
    "# Normalization helpers \n",
    "# ----------------------------\n",
    "def apply_normalization(data, method, norm_stats, var_base, resolution='lr_interp'):\n",
    "    \"\"\"Apply normalization method to data.\"\"\"\n",
    "    if method == 'none':\n",
    "        return data\n",
    "    \n",
    "    elif method == 'minmax_global':\n",
    "        data_min = norm_stats[var_base][resolution]['global_min']\n",
    "        data_max = norm_stats[var_base][resolution]['global_max']\n",
    "        return 2 * (data - data_min) / (data_max - data_min + 1e-8) - 1\n",
    "    \n",
    "    elif method == 'minmax_pixel':\n",
    "        data_min = norm_stats[var_base][resolution]['pixel_min']\n",
    "        data_max = norm_stats[var_base][resolution]['pixel_max']\n",
    "        return 2 * (data - data_min) / (data_max - data_min + 1e-8) - 1\n",
    "    \n",
    "    elif method == 'zscore_global':\n",
    "        data_mean = norm_stats[var_base][resolution]['global_mean']\n",
    "        data_std = norm_stats[var_base][resolution]['global_std']\n",
    "        return (data - data_mean) / (data_std + 1e-8)\n",
    "    \n",
    "    elif method == 'zscore_pixel':\n",
    "        data_mean = norm_stats[var_base][resolution]['pixel_mean']\n",
    "        data_std = norm_stats[var_base][resolution]['pixel_std']\n",
    "        return (data - data_mean) / (data_std + 1e-8)\n",
    "    \n",
    "    elif method == 'instance_zscore':\n",
    "        data_np = data.values if hasattr(data, 'values') else data\n",
    "        data_mean = np.mean(data_np, axis=(1, 2), keepdims=True)\n",
    "        data_std = np.std(data_np, axis=(1, 2), keepdims=True)\n",
    "        data_normalized = (data_np - data_mean) / (data_std + 1e-8)\n",
    "        return data_normalized\n",
    "    \n",
    "    elif method == 'instance_minmax':\n",
    "        data_np = data.values if hasattr(data, 'values') else data\n",
    "        data_min = np.min(data_np, axis=(1, 2), keepdims=True)\n",
    "        data_max = np.max(data_np, axis=(1, 2), keepdims=True)\n",
    "        data_normalized = 2 * (data_np - data_min) / (data_max - data_min + 1e-8) - 1\n",
    "        return data_normalized\n",
    "\n",
    "def denormalize_predictions(predictions, method, norm_stats, var_base, input_data=None):\n",
    "    \"\"\"Denormalize predictions back to original scale.\"\"\"\n",
    "    if method == 'none':\n",
    "        return predictions\n",
    "    \n",
    "    elif method == 'minmax_global':\n",
    "        hr_min = norm_stats[var_base]['hr']['global_min']\n",
    "        hr_max = norm_stats[var_base]['hr']['global_max']\n",
    "        return ((predictions + 1) / 2) * (hr_max - hr_min) + hr_min\n",
    "    \n",
    "    elif method == 'minmax_pixel':\n",
    "        hr_min = norm_stats[var_base]['hr']['pixel_min']\n",
    "        hr_max = norm_stats[var_base]['hr']['pixel_max']\n",
    "        return ((predictions + 1) / 2) * (hr_max - hr_min) + hr_min\n",
    "    \n",
    "    elif method == 'zscore_global':\n",
    "        hr_mean = norm_stats[var_base]['hr']['global_mean']\n",
    "        hr_std = norm_stats[var_base]['hr']['global_std']\n",
    "        return predictions * hr_std + hr_mean\n",
    "    \n",
    "    elif method == 'zscore_pixel':\n",
    "        hr_mean = norm_stats[var_base]['hr']['pixel_mean']\n",
    "        hr_std = norm_stats[var_base]['hr']['pixel_std']\n",
    "        return predictions * hr_std + hr_mean\n",
    "    \n",
    "    elif method == 'instance_zscore':\n",
    "        input_mean = np.mean(input_data, axis=(1, 2), keepdims=True)\n",
    "        input_std = np.std(input_data, axis=(1, 2), keepdims=True)\n",
    "        return predictions * input_std + input_mean\n",
    "    \n",
    "    elif method == 'instance_minmax':\n",
    "        input_min = np.min(input_data, axis=(1, 2), keepdims=True)\n",
    "        input_max = np.max(input_data, axis=(1, 2), keepdims=True)\n",
    "        return ((predictions + 1) / 2) * (input_max - input_min) + input_min\n",
    "\n",
    "# ----------------------------\n",
    "# Model evaluation\n",
    "# ----------------------------\n",
    "def evaluate_model(model_path, norm_method, dataset, test_period):\n",
    "    \"\"\"Evaluate a single model on a dataset.\"\"\"\n",
    "    print(f\"    Evaluating {norm_method}...\")\n",
    "    \n",
    "    # Load model\n",
    "    model = UNet(in_channels=1, out_channels=1, initial_features=32, depth=5, dropout=0.2)\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Extract test data\n",
    "    lr_data_xr = dataset[var_lr].sel(time=slice(test_period[0], test_period[1]))\n",
    "    hr_data_xr = dataset[variable].sel(time=slice(test_period[0], test_period[1]))\n",
    "    \n",
    "    lr_data_raw = lr_data_xr.values\n",
    "    hr_data_raw = hr_data_xr.values\n",
    "    \n",
    "    # Apply normalization to input\n",
    "    lr_data_norm = apply_normalization(lr_data_xr, norm_method, norm_stats, var_base)\n",
    "    \n",
    "    # Ensure it's numpy\n",
    "    if hasattr(lr_data_norm, 'values'):\n",
    "        lr_data_norm = lr_data_norm.values\n",
    "    \n",
    "    # Predict in batches\n",
    "    batch_size = 32\n",
    "    n_samples = len(lr_data_norm)\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, n_samples, batch_size):\n",
    "            batch = lr_data_norm[i:i+batch_size]\n",
    "            batch_tensor = torch.tensor(batch, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "            \n",
    "            batch_pred = model(batch_tensor)\n",
    "            predictions.append(batch_pred.cpu().numpy())\n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0).squeeze(1)\n",
    "    \n",
    "    # Denormalize predictions\n",
    "    predictions_denorm = denormalize_predictions(\n",
    "        predictions, norm_method, norm_stats, var_base, lr_data_raw\n",
    "    )\n",
    "    \n",
    "    return predictions_denorm, hr_data_raw, lr_data_raw\n",
    "\n",
    "# ----------------------------\n",
    "# Main evaluation loop\n",
    "# ----------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATING PRECIPITATION (PR) MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for scenario_name, dataset in datasets.items():\n",
    "    print(f\"\\n{scenario_name.upper()} Scenario\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    test_period = test_periods[scenario_name]\n",
    "    scenario_results = {}\n",
    "    \n",
    "    # Get ground truth\n",
    "    hr_data = dataset[variable].sel(time=slice(test_period[0], test_period[1]))\n",
    "    lr_data = dataset[var_lr].sel(time=slice(test_period[0], test_period[1]))\n",
    "    time_coords = hr_data.time\n",
    "    lat_coords = hr_data.lat\n",
    "    lon_coords = hr_data.lon\n",
    "    \n",
    "    # Store ground truth and input\n",
    "    scenario_results['groundtruth'] = hr_data\n",
    "    scenario_results['input'] = lr_data\n",
    "    \n",
    "    # Evaluate each model\n",
    "    for norm_method in normalizations:\n",
    "        model_path = ckpt_dir / f\"pr_{norm_method}.pth\"\n",
    "        \n",
    "        if not model_path.exists():\n",
    "            print(f\"Model not found: {model_path}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            predictions, groundtruth, inputs = evaluate_model(\n",
    "                model_path, norm_method, dataset, test_period\n",
    "            )\n",
    "            \n",
    "            pred_da = xr.DataArray(\n",
    "                predictions,\n",
    "                coords={'time': time_coords, 'lat': lat_coords, 'lon': lon_coords},\n",
    "                dims=['time', 'lat', 'lon'],\n",
    "                name=f'pr_pred_{norm_method}'\n",
    "            )\n",
    "            \n",
    "            scenario_results[f'pred_{norm_method}'] = pred_da\n",
    "            print(f\"Success\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            continue\n",
    "    \n",
    "    all_results[scenario_name] = scenario_results\n",
    "\n",
    "# ----------------------------\n",
    "# Save results\n",
    "# ----------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "output_dir = Path(\"evaluation_results\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for scenario_name, results in all_results.items():\n",
    "    ds_result = xr.Dataset()\n",
    "    \n",
    "    ds_result['groundtruth'] = results['groundtruth']\n",
    "    ds_result['input'] = results['input']\n",
    "    \n",
    "    for norm_method in normalizations:\n",
    "        key = f'pred_{norm_method}'\n",
    "        if key in results:\n",
    "            ds_result[key] = results[key]\n",
    "    \n",
    "    output_path = output_dir / f\"pr_evaluation_{scenario_name}.nc\"\n",
    "    ds_result.to_netcdf(output_path)\n",
    "    print(f\"Saved: {output_path}\")\n",
    "\n",
    "print(\"\\nEvaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26db45d0-5820-42b9-bd18-59dae3fc0a89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
