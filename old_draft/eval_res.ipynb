{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6fd846-a88d-41ed-abb0-cc7b3847fe66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from unet import UNet\n",
    "from scipy import stats\n",
    "\n",
    "# Configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data_dir = Path(\"data\")\n",
    "ckpt_dir = Path(\"ckpts\")\n",
    "\n",
    "# Test periods - ADDED g6sulfur\n",
    "test_periods = {\n",
    "    'historical': ('2001', '2014'),\n",
    "    'ssp126': ('2015', '2100'),\n",
    "    'ssp245': ('2015', '2100'), \n",
    "    'ssp585': ('2015', '2100'),\n",
    "    'g6sulfur': ('2020', '2099')  # Added\n",
    "}\n",
    "\n",
    "# Variables and input types to evaluate\n",
    "variables = ['pr', 'tas']\n",
    "input_types = ['raw', 'gma', 'grid', 'gmt']  # Simplified names\n",
    "\n",
    "# Load datasets - ADDED g6sulfur\n",
    "print(\"Loading datasets...\")\n",
    "datasets = {\n",
    "    'historical': xr.open_dataset(data_dir / \"MPI-ESM1-2-HR-LR_historical_residual_detrended.nc\"),\n",
    "    'ssp126': xr.open_dataset(data_dir / \"MPI-ESM1-2-HR-LR_ssp126_residual_detrended.nc\"),\n",
    "    'ssp245': xr.open_dataset(data_dir / \"MPI-ESM1-2-HR-LR_ssp245_residual_detrended.nc\"),\n",
    "    'ssp585': xr.open_dataset(data_dir / \"MPI-ESM1-2-HR-LR_ssp585_residual_detrended.nc\"),\n",
    "    'g6sulfur': xr.open_dataset(data_dir / \"MPI-ESM1-2-HR-LR_g6sulfur_residual_detrended.nc\")  # Added\n",
    "}\n",
    "\n",
    "# Load original datasets for undetrended versions - ADDED g6sulfur\n",
    "datasets_original = {\n",
    "    'historical': xr.open_dataset(data_dir / \"MPI-ESM1-2-HR-LR_historical_r1i1p1f1_1850_2014_allvars.nc\"),\n",
    "    'ssp126': xr.open_dataset(data_dir / \"MPI-ESM1-2-HR-LR_ssp126_r1i1p1f1_2015_2100_allvars.nc\"),\n",
    "    'ssp245': xr.open_dataset(data_dir / \"MPI-ESM1-2-HR-LR_ssp245_r1i1p1f1_2015_2100_allvars.nc\"),\n",
    "    'ssp585': xr.open_dataset(data_dir / \"MPI-ESM1-2-HR-LR_ssp585_r1i1p1f1_2015_2100_allvars.nc\"),\n",
    "    'g6sulfur': xr.open_dataset(data_dir / \"MPI-ESM1-2-HR-LR_g6sulfur_r1i1p1f1_2020_2099_allvars.nc\")  # Added\n",
    "}\n",
    "\n",
    "# Load normalization statistics\n",
    "print(\"Loading normalization statistics...\")\n",
    "with open(data_dir / \"norm_stats_zscore_pixel_residual_detrended.pkl\", 'rb') as f:\n",
    "    norm_stats = pickle.load(f)\n",
    "\n",
    "def evaluate_model(model_path, var, input_type, dataset, dataset_original, test_period):\n",
    "    \"\"\"Evaluate a single model on a dataset.\"\"\"\n",
    "    \n",
    "    # Load model\n",
    "    model = UNet(in_channels=1, out_channels=1, initial_features=32, depth=5, dropout=0.2)\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Get variable names based on input type\n",
    "    if input_type == 'raw':\n",
    "        input_var = f\"{var}_lr_interp\"\n",
    "        input_key = 'lr_interp'\n",
    "    else:\n",
    "        # Map simplified names back to full detrend names\n",
    "        detrend_map = {'gma': 'detrend_gma', 'grid': 'detrend_grid', 'gmt': 'detrend_gmt'}\n",
    "        input_var = f\"{var}_lr_{detrend_map[input_type]}\"\n",
    "        input_key = f'lr_{detrend_map[input_type]}'\n",
    "    \n",
    "    # Extract test data\n",
    "    lr_data = dataset[input_var].sel(time=slice(test_period[0], test_period[1])).values\n",
    "    residual_true = dataset[f\"{var}_residual\"].sel(time=slice(test_period[0], test_period[1])).values\n",
    "    lr_original = dataset_original[f\"{var}_lr_interp\"].sel(time=slice(test_period[0], test_period[1])).values\n",
    "    hr_original = dataset_original[f\"{var}_hr\"].sel(time=slice(test_period[0], test_period[1])).values\n",
    "    \n",
    "    # Apply zscore_pixel normalization to input\n",
    "    lr_mean = norm_stats[var][input_key]['pixel_mean']\n",
    "    lr_std = norm_stats[var][input_key]['pixel_std']\n",
    "    lr_normalized = (lr_data - lr_mean) / (lr_std + 1e-8)\n",
    "    \n",
    "    # Predict in batches\n",
    "    batch_size = 32\n",
    "    n_samples = len(lr_normalized)\n",
    "    predictions_norm = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, n_samples, batch_size):\n",
    "            batch = lr_normalized[i:i+batch_size]\n",
    "            batch_tensor = torch.tensor(batch, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "            batch_pred = model(batch_tensor)\n",
    "            predictions_norm.append(batch_pred.cpu().numpy())\n",
    "    \n",
    "    predictions_norm = np.concatenate(predictions_norm, axis=0).squeeze(1)\n",
    "    \n",
    "    # Denormalize residual predictions\n",
    "    residual_mean = norm_stats[var]['residual']['pixel_mean']\n",
    "    residual_std = norm_stats[var]['residual']['pixel_std']\n",
    "    residual_pred = predictions_norm * residual_std + residual_mean\n",
    "    \n",
    "    # Reconstruct full HR prediction by adding back LR_interp\n",
    "    hr_pred = residual_pred + lr_original\n",
    "    \n",
    "    return hr_pred, hr_original, lr_original, residual_pred, residual_true\n",
    "\n",
    "# Main evaluation loop\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATING RESIDUAL MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for scenario_name in datasets.keys():\n",
    "    print(f\"\\n{scenario_name.upper()} Scenario\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    dataset = datasets[scenario_name]\n",
    "    dataset_original = datasets_original[scenario_name]\n",
    "    test_period = test_periods[scenario_name]\n",
    "    scenario_results = {}\n",
    "    \n",
    "    for var in variables:\n",
    "        print(f\"\\n  Variable: {var}\")\n",
    "        var_results = {}\n",
    "        \n",
    "        # Get coordinates for creating xarray DataArrays\n",
    "        time_coords = dataset[f\"{var}_hr\"].sel(time=slice(test_period[0], test_period[1])).time\n",
    "        lat_coords = dataset[f\"{var}_hr\"].lat\n",
    "        lon_coords = dataset[f\"{var}_hr\"].lon\n",
    "        \n",
    "        # Store ground truth (original HR, not residual)\n",
    "        hr_true = dataset_original[f\"{var}_hr\"].sel(time=slice(test_period[0], test_period[1]))\n",
    "        lr_input = dataset_original[f\"{var}_lr_interp\"].sel(time=slice(test_period[0], test_period[1]))\n",
    "        \n",
    "        var_results['groundtruth'] = hr_true\n",
    "        var_results['input'] = lr_input\n",
    "        \n",
    "        for input_type in input_types:\n",
    "            # Build model filename based on your naming convention\n",
    "            if input_type == 'raw':\n",
    "                model_filename = f\"{var}_lr_to_residual.pth\"\n",
    "            else:\n",
    "                model_filename = f\"{var}_lr_{input_type}_to_residual.pth\"\n",
    "            \n",
    "            model_path = ckpt_dir / model_filename\n",
    "            \n",
    "            if not model_path.exists():\n",
    "                print(f\"    Model not found: {model_path}\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                print(f\"    Evaluating {input_type}...\", end=\" \")\n",
    "                \n",
    "                hr_pred, hr_true_np, lr_orig, residual_pred, residual_true = evaluate_model(\n",
    "                    model_path, var, input_type, dataset, dataset_original, test_period\n",
    "                )\n",
    "                \n",
    "                # Create xarray DataArray for predictions\n",
    "                pred_da = xr.DataArray(\n",
    "                    hr_pred,\n",
    "                    coords={'time': time_coords, 'lat': lat_coords, 'lon': lon_coords},\n",
    "                    dims=['time', 'lat', 'lon'],\n",
    "                    name=f'{var}_pred_{input_type}'\n",
    "                )\n",
    "                \n",
    "                var_results[f'pred_{input_type}'] = pred_da\n",
    "                print(\"Success\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                continue\n",
    "        \n",
    "        scenario_results[var] = var_results\n",
    "    \n",
    "    all_results[scenario_name] = scenario_results\n",
    "\n",
    "# Save results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "output_dir = Path(\"evaluation_results_residual\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for scenario_name, scenario_results in all_results.items():\n",
    "    for var, var_results in scenario_results.items():\n",
    "        ds_result = xr.Dataset()\n",
    "        \n",
    "        # Store ground truth and input\n",
    "        ds_result['groundtruth'] = var_results['groundtruth']\n",
    "        ds_result['input'] = var_results['input']\n",
    "        \n",
    "        # Store predictions\n",
    "        for input_type in input_types:\n",
    "            key = f'pred_{input_type}'\n",
    "            if key in var_results:\n",
    "                ds_result[key] = var_results[key]\n",
    "        \n",
    "        # Save to netCDF\n",
    "        output_path = output_dir / f\"{var}_evaluation_{scenario_name}.nc\"\n",
    "        ds_result.to_netcdf(output_path)\n",
    "        print(f\"Saved: {output_path}\")\n",
    "\n",
    "print(\"\\nEvaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760875cb-760b-4ebc-ad80-e6392ff3f366",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ad1941f-2905-4f6b-a31a-febefe6dd25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Loading normalization statistics...\n",
      "\n",
      "================================================================================\n",
      "EVALUATING RESIDUAL MODELS\n",
      "================================================================================\n",
      "\n",
      "HISTORICAL Scenario\n",
      "----------------------------------------\n",
      "\n",
      "  Variable: pr\n",
      "    Evaluating raw... Success\n",
      "    Evaluating gma... Success\n",
      "    Evaluating grid... Success\n",
      "    Evaluating gmt... Success\n",
      "\n",
      "  Variable: tas\n",
      "    Evaluating raw... Success\n",
      "    Evaluating gma... Success\n",
      "    Evaluating grid... Success\n",
      "    Evaluating gmt... Success\n",
      "\n",
      "SSP126 Scenario\n",
      "----------------------------------------\n",
      "\n",
      "  Variable: pr\n",
      "    Evaluating raw... Success\n",
      "    Evaluating gma... Success\n",
      "    Evaluating grid... Success\n",
      "    Evaluating gmt... Success\n",
      "\n",
      "  Variable: tas\n",
      "    Evaluating raw... Success\n",
      "    Evaluating gma... Success\n",
      "    Evaluating grid... Success\n",
      "    Evaluating gmt... Success\n",
      "\n",
      "SSP245 Scenario\n",
      "----------------------------------------\n",
      "\n",
      "  Variable: pr\n",
      "    Evaluating raw... Success\n",
      "    Evaluating gma... Success\n",
      "    Evaluating grid... Success\n",
      "    Evaluating gmt... Success\n",
      "\n",
      "  Variable: tas\n",
      "    Evaluating raw... Success\n",
      "    Evaluating gma... Success\n",
      "    Evaluating grid... Success\n",
      "    Evaluating gmt... Success\n",
      "\n",
      "SSP585 Scenario\n",
      "----------------------------------------\n",
      "\n",
      "  Variable: pr\n",
      "    Evaluating raw... Success\n",
      "    Evaluating gma... Success\n",
      "    Evaluating grid... Success\n",
      "    Evaluating gmt... Success\n",
      "\n",
      "  Variable: tas\n",
      "    Evaluating raw... Success\n",
      "    Evaluating gma... Success\n",
      "    Evaluating grid... Success\n",
      "    Evaluating gmt... Success\n",
      "\n",
      "================================================================================\n",
      "SAVING RESULTS\n",
      "================================================================================\n",
      "Saved: evaluation_results_residual/pr_evaluation_historical.nc\n",
      "Saved: evaluation_results_residual/tas_evaluation_historical.nc\n",
      "Saved: evaluation_results_residual/pr_evaluation_ssp126.nc\n",
      "Saved: evaluation_results_residual/tas_evaluation_ssp126.nc\n",
      "Saved: evaluation_results_residual/pr_evaluation_ssp245.nc\n",
      "Saved: evaluation_results_residual/tas_evaluation_ssp245.nc\n",
      "Saved: evaluation_results_residual/pr_evaluation_ssp585.nc\n",
      "Saved: evaluation_results_residual/tas_evaluation_ssp585.nc\n",
      "\n",
      "Evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from unet import UNet\n",
    "from scipy import stats\n",
    "\n",
    "# Configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data_dir = Path(\"data\")\n",
    "ckpt_dir = Path(\"ckpts\")\n",
    "\n",
    "# Test periods\n",
    "test_periods = {\n",
    "    'historical': ('2001', '2014'),\n",
    "    'ssp126': ('2015', '2100'),\n",
    "    'ssp245': ('2015', '2100'), \n",
    "    'ssp585': ('2015', '2100')\n",
    "}\n",
    "\n",
    "# Variables and input types to evaluate\n",
    "variables = ['pr', 'tas']\n",
    "input_types = ['raw', 'gma', 'grid', 'gmt']  # Simplified names\n",
    "\n",
    "# Load datasets\n",
    "print(\"Loading datasets...\")\n",
    "datasets = {\n",
    "    'historical': xr.open_dataset(data_dir / \"MPI-ESM1-2-HR-LR_historical_residual_detrended.nc\"),\n",
    "    'ssp126': xr.open_dataset(data_dir / \"MPI-ESM1-2-HR-LR_ssp126_residual_detrended.nc\"),\n",
    "    'ssp245': xr.open_dataset(data_dir / \"MPI-ESM1-2-HR-LR_ssp245_residual_detrended.nc\"),\n",
    "    'ssp585': xr.open_dataset(data_dir / \"MPI-ESM1-2-HR-LR_ssp585_residual_detrended.nc\")\n",
    "}\n",
    "\n",
    "# Load original datasets for undetrended versions\n",
    "datasets_original = {\n",
    "    'historical': xr.open_dataset(data_dir / \"MPI-ESM1-2-HR-LR_historical_r1i1p1f1_1850_2014_allvars.nc\"),\n",
    "    'ssp126': xr.open_dataset(data_dir / \"MPI-ESM1-2-HR-LR_ssp126_r1i1p1f1_2015_2100_allvars.nc\"),\n",
    "    'ssp245': xr.open_dataset(data_dir / \"MPI-ESM1-2-HR-LR_ssp245_r1i1p1f1_2015_2100_allvars.nc\"),\n",
    "    'ssp585': xr.open_dataset(data_dir / \"MPI-ESM1-2-HR-LR_ssp585_r1i1p1f1_2015_2100_allvars.nc\")\n",
    "}\n",
    "\n",
    "# Load normalization statistics\n",
    "print(\"Loading normalization statistics...\")\n",
    "with open(data_dir / \"norm_stats_zscore_pixel_residual_detrended.pkl\", 'rb') as f:\n",
    "    norm_stats = pickle.load(f)\n",
    "\n",
    "def evaluate_model(model_path, var, input_type, dataset, dataset_original, test_period):\n",
    "    \"\"\"Evaluate a single model on a dataset.\"\"\"\n",
    "    \n",
    "    # Load model\n",
    "    model = UNet(in_channels=1, out_channels=1, initial_features=32, depth=5, dropout=0.2)\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Get variable names based on input type\n",
    "    if input_type == 'raw':\n",
    "        input_var = f\"{var}_lr_interp\"\n",
    "        input_key = 'lr_interp'\n",
    "    else:\n",
    "        # Map simplified names back to full detrend names\n",
    "        detrend_map = {'gma': 'detrend_gma', 'grid': 'detrend_grid', 'gmt': 'detrend_gmt'}\n",
    "        input_var = f\"{var}_lr_{detrend_map[input_type]}\"\n",
    "        input_key = f'lr_{detrend_map[input_type]}'\n",
    "    \n",
    "    # Extract test data\n",
    "    lr_data = dataset[input_var].sel(time=slice(test_period[0], test_period[1])).values\n",
    "    residual_true = dataset[f\"{var}_residual\"].sel(time=slice(test_period[0], test_period[1])).values\n",
    "    lr_original = dataset_original[f\"{var}_lr_interp\"].sel(time=slice(test_period[0], test_period[1])).values\n",
    "    hr_original = dataset_original[f\"{var}_hr\"].sel(time=slice(test_period[0], test_period[1])).values\n",
    "    \n",
    "    # Apply zscore_pixel normalization to input\n",
    "    lr_mean = norm_stats[var][input_key]['pixel_mean']\n",
    "    lr_std = norm_stats[var][input_key]['pixel_std']\n",
    "    lr_normalized = (lr_data - lr_mean) / (lr_std + 1e-8)\n",
    "    \n",
    "    # Predict in batches\n",
    "    batch_size = 32\n",
    "    n_samples = len(lr_normalized)\n",
    "    predictions_norm = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, n_samples, batch_size):\n",
    "            batch = lr_normalized[i:i+batch_size]\n",
    "            batch_tensor = torch.tensor(batch, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "            batch_pred = model(batch_tensor)\n",
    "            predictions_norm.append(batch_pred.cpu().numpy())\n",
    "    \n",
    "    predictions_norm = np.concatenate(predictions_norm, axis=0).squeeze(1)\n",
    "    \n",
    "    # Denormalize residual predictions\n",
    "    residual_mean = norm_stats[var]['residual']['pixel_mean']\n",
    "    residual_std = norm_stats[var]['residual']['pixel_std']\n",
    "    residual_pred = predictions_norm * residual_std + residual_mean\n",
    "    \n",
    "    # Reconstruct full HR prediction by adding back LR_interp\n",
    "    hr_pred = residual_pred + lr_original\n",
    "    \n",
    "    return hr_pred, hr_original, lr_original, residual_pred, residual_true\n",
    "\n",
    "# Main evaluation loop\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATING RESIDUAL MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for scenario_name in datasets.keys():\n",
    "    print(f\"\\n{scenario_name.upper()} Scenario\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    dataset = datasets[scenario_name]\n",
    "    dataset_original = datasets_original[scenario_name]\n",
    "    test_period = test_periods[scenario_name]\n",
    "    scenario_results = {}\n",
    "    \n",
    "    for var in variables:\n",
    "        print(f\"\\n  Variable: {var}\")\n",
    "        var_results = {}\n",
    "        \n",
    "        # Get coordinates for creating xarray DataArrays\n",
    "        time_coords = dataset[f\"{var}_hr\"].sel(time=slice(test_period[0], test_period[1])).time\n",
    "        lat_coords = dataset[f\"{var}_hr\"].lat\n",
    "        lon_coords = dataset[f\"{var}_hr\"].lon\n",
    "        \n",
    "        # Store ground truth (original HR, not residual)\n",
    "        hr_true = dataset_original[f\"{var}_hr\"].sel(time=slice(test_period[0], test_period[1]))\n",
    "        lr_input = dataset_original[f\"{var}_lr_interp\"].sel(time=slice(test_period[0], test_period[1]))\n",
    "        \n",
    "        var_results['groundtruth'] = hr_true\n",
    "        var_results['input'] = lr_input\n",
    "        \n",
    "        for input_type in input_types:\n",
    "            # Build model filename based on your naming convention\n",
    "            if input_type == 'raw':\n",
    "                model_filename = f\"{var}_lr_to_residual.pth\"\n",
    "            else:\n",
    "                model_filename = f\"{var}_lr_{input_type}_to_residual.pth\"\n",
    "            \n",
    "            model_path = ckpt_dir / model_filename\n",
    "            \n",
    "            if not model_path.exists():\n",
    "                print(f\"    Model not found: {model_path}\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                print(f\"    Evaluating {input_type}...\", end=\" \")\n",
    "                \n",
    "                hr_pred, hr_true_np, lr_orig, residual_pred, residual_true = evaluate_model(\n",
    "                    model_path, var, input_type, dataset, dataset_original, test_period\n",
    "                )\n",
    "                \n",
    "                # Create xarray DataArray for predictions\n",
    "                pred_da = xr.DataArray(\n",
    "                    hr_pred,\n",
    "                    coords={'time': time_coords, 'lat': lat_coords, 'lon': lon_coords},\n",
    "                    dims=['time', 'lat', 'lon'],\n",
    "                    name=f'{var}_pred_{input_type}'\n",
    "                )\n",
    "                \n",
    "                var_results[f'pred_{input_type}'] = pred_da\n",
    "                print(\"Success\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                continue\n",
    "        \n",
    "        scenario_results[var] = var_results\n",
    "    \n",
    "    all_results[scenario_name] = scenario_results\n",
    "\n",
    "# Save results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "output_dir = Path(\"evaluation_results_residual\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for scenario_name, scenario_results in all_results.items():\n",
    "    for var, var_results in scenario_results.items():\n",
    "        ds_result = xr.Dataset()\n",
    "        \n",
    "        # Store ground truth and input\n",
    "        ds_result['groundtruth'] = var_results['groundtruth']\n",
    "        ds_result['input'] = var_results['input']\n",
    "        \n",
    "        # Store predictions\n",
    "        for input_type in input_types:\n",
    "            key = f'pred_{input_type}'\n",
    "            if key in var_results:\n",
    "                ds_result[key] = var_results[key]\n",
    "        \n",
    "        # Save to netCDF\n",
    "        output_path = output_dir / f\"{var}_evaluation_{scenario_name}.nc\"\n",
    "        ds_result.to_netcdf(output_path)\n",
    "        print(f\"Saved: {output_path}\")\n",
    "\n",
    "print(\"\\nEvaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26db45d0-5820-42b9-bd18-59dae3fc0a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading G6sulfur datasets...\n",
      "Loading normalization statistics...\n",
      "\n",
      "================================================================================\n",
      "EVALUATING G6SULFUR RESIDUAL MODELS\n",
      "================================================================================\n",
      "Evaluating raw... Success\n",
      "Evaluating gma... Success\n",
      "Evaluating grid... Success\n",
      "Evaluating gmt... Success\n",
      "\n",
      "================================================================================\n",
      "SAVING G6SULFUR RESULTS\n",
      "================================================================================\n",
      "   Saved: evaluation_results_residual/tas_evaluation_g6sulfur.nc\n",
      "   Variables: ['groundtruth', 'input', 'pred_raw', 'pred_gma', 'pred_grid', 'pred_gmt']\n",
      "   Time range: 2020 to 2099\n",
      "\n",
      "G6sulfur residual evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from unet import UNet\n",
    "\n",
    "# Configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data_dir = Path(\"data\")\n",
    "ckpt_dir = Path(\"ckpts\")\n",
    "\n",
    "# Test period for G6sulfur\n",
    "test_period_g6 = ('2020', '2099')\n",
    "\n",
    "# Variable and input types\n",
    "var = 'tas'\n",
    "input_types = ['raw', 'gma', 'grid', 'gmt']\n",
    "\n",
    "# Load datasets\n",
    "print(\"Loading G6sulfur datasets...\")\n",
    "dataset = xr.open_dataset(data_dir / \"MPI-ESM1-2-HR-LR_g6sulfur_residual_detrended.nc\")\n",
    "dataset_original = xr.open_dataset(data_dir / \"MPI-ESM1-2-HR-LR_g6sulfur_r1i1p1f1_2020_2099_allvars.nc\")\n",
    "\n",
    "# Load normalization statistics\n",
    "print(\"Loading normalization statistics...\")\n",
    "with open(data_dir / \"norm_stats_zscore_pixel_residual_detrended.pkl\", 'rb') as f:\n",
    "    norm_stats = pickle.load(f)\n",
    "\n",
    "def evaluate_model(model_path, var, input_type, dataset, dataset_original, test_period):\n",
    "    \"\"\"Evaluate a single model on a dataset.\"\"\"\n",
    "    \n",
    "    # Load model\n",
    "    model = UNet(in_channels=1, out_channels=1, initial_features=32, depth=5, dropout=0.2)\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Get variable names based on input type\n",
    "    if input_type == 'raw':\n",
    "        input_var = f\"{var}_lr_interp\"\n",
    "        input_key = 'lr_interp'\n",
    "    else:\n",
    "        # Map simplified names back to full detrend names\n",
    "        detrend_map = {'gma': 'detrend_gma', 'grid': 'detrend_grid', 'gmt': 'detrend_gmt'}\n",
    "        input_var = f\"{var}_lr_{detrend_map[input_type]}\"\n",
    "        input_key = f'lr_{detrend_map[input_type]}'\n",
    "    \n",
    "    # Extract test data\n",
    "    lr_data = dataset[input_var].sel(time=slice(test_period[0], test_period[1])).values\n",
    "    residual_true = dataset[f\"{var}_residual\"].sel(time=slice(test_period[0], test_period[1])).values\n",
    "    lr_original = dataset_original[f\"{var}_lr_interp\"].sel(time=slice(test_period[0], test_period[1])).values\n",
    "    hr_original = dataset_original[f\"{var}_hr\"].sel(time=slice(test_period[0], test_period[1])).values\n",
    "    \n",
    "    # Apply zscore_pixel normalization to input\n",
    "    lr_mean = norm_stats[var][input_key]['pixel_mean']\n",
    "    lr_std = norm_stats[var][input_key]['pixel_std']\n",
    "    lr_normalized = (lr_data - lr_mean) / (lr_std + 1e-8)\n",
    "    \n",
    "    # Predict in batches\n",
    "    batch_size = 32\n",
    "    n_samples = len(lr_normalized)\n",
    "    predictions_norm = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, n_samples, batch_size):\n",
    "            batch = lr_normalized[i:i+batch_size]\n",
    "            batch_tensor = torch.tensor(batch, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "            batch_pred = model(batch_tensor)\n",
    "            predictions_norm.append(batch_pred.cpu().numpy())\n",
    "    \n",
    "    predictions_norm = np.concatenate(predictions_norm, axis=0).squeeze(1)\n",
    "    \n",
    "    # Denormalize residual predictions\n",
    "    residual_mean = norm_stats[var]['residual']['pixel_mean']\n",
    "    residual_std = norm_stats[var]['residual']['pixel_std']\n",
    "    residual_pred = predictions_norm * residual_std + residual_mean\n",
    "    \n",
    "    # Reconstruct full HR prediction by adding back LR_interp\n",
    "    hr_pred = residual_pred + lr_original\n",
    "    \n",
    "    return hr_pred, hr_original, lr_original, residual_pred, residual_true\n",
    "\n",
    "# Main evaluation for G6sulfur\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATING G6SULFUR RESIDUAL MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "g6_results = {}\n",
    "\n",
    "# Get coordinates for creating xarray DataArrays\n",
    "time_coords = dataset[f\"{var}_hr\"].sel(time=slice(test_period_g6[0], test_period_g6[1])).time\n",
    "lat_coords = dataset[f\"{var}_hr\"].lat\n",
    "lon_coords = dataset[f\"{var}_hr\"].lon\n",
    "\n",
    "# Store ground truth (original HR, not residual)\n",
    "hr_true = dataset_original[f\"{var}_hr\"].sel(time=slice(test_period_g6[0], test_period_g6[1]))\n",
    "lr_input = dataset_original[f\"{var}_lr_interp\"].sel(time=slice(test_period_g6[0], test_period_g6[1]))\n",
    "\n",
    "g6_results['groundtruth'] = hr_true\n",
    "g6_results['input'] = lr_input\n",
    "\n",
    "for input_type in input_types:\n",
    "    # Build model filename\n",
    "    if input_type == 'raw':\n",
    "        model_filename = f\"{var}_lr_to_residual.pth\"\n",
    "    else:\n",
    "        model_filename = f\"{var}_lr_{input_type}_to_residual.pth\"\n",
    "    \n",
    "    model_path = ckpt_dir / model_filename\n",
    "    \n",
    "    if not model_path.exists():\n",
    "        print(f\"Model not found: {model_path}\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        print(f\"Evaluating {input_type}...\", end=\" \")\n",
    "        \n",
    "        hr_pred, hr_true_np, lr_orig, residual_pred, residual_true = evaluate_model(\n",
    "            model_path, var, input_type, dataset, dataset_original, test_period_g6\n",
    "        )\n",
    "        \n",
    "        # Create xarray DataArray for predictions\n",
    "        pred_da = xr.DataArray(\n",
    "            hr_pred,\n",
    "            coords={'time': time_coords, 'lat': lat_coords, 'lon': lon_coords},\n",
    "            dims=['time', 'lat', 'lon'],\n",
    "            name=f'{var}_pred_{input_type}'\n",
    "        )\n",
    "        \n",
    "        g6_results[f'pred_{input_type}'] = pred_da\n",
    "        print(\"Success\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        continue\n",
    "\n",
    "# Save results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING G6SULFUR RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "output_dir = Path(\"evaluation_results_residual\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "ds_result = xr.Dataset()\n",
    "\n",
    "# Store ground truth and input\n",
    "ds_result['groundtruth'] = g6_results['groundtruth']\n",
    "ds_result['input'] = g6_results['input']\n",
    "\n",
    "# Store predictions\n",
    "for input_type in input_types:\n",
    "    key = f'pred_{input_type}'\n",
    "    if key in g6_results:\n",
    "        ds_result[key] = g6_results[key]\n",
    "\n",
    "# Save to netCDF\n",
    "output_path = output_dir / f\"{var}_evaluation_g6sulfur.nc\"\n",
    "ds_result.to_netcdf(output_path)\n",
    "print(f\"   Saved: {output_path}\")\n",
    "print(f\"   Variables: {list(ds_result.data_vars)}\")\n",
    "print(f\"   Time range: {test_period_g6[0]} to {test_period_g6[1]}\")\n",
    "\n",
    "print(\"\\nG6sulfur residual evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343df88a-d924-447c-abda-17610ae6598b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
